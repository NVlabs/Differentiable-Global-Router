Review #1
Have a Question to Authors? (Optional)
How is the running time compared with [2, 8, 14]? How does the method compare to [2, 8, 14] on wirelength, via, and DRC count after detailed routing?

We appreciate the questions from reviewer!
The running time comparison is shown in Figure 5(a). From the figure, we can see that when the number of nets is relatively small, i.e., less than 100k, all methods are fast, although DGR is the slowest (less than 20 seconds) due to the runtime cost of training. When the size increases, both SPRoute2.0 [8] and Yao [14] 	has a more significant runtime slope, while CUGR2 [2] and DGR has a lower runtime increase and faster than [8] and [14]. when the design complexity continues increasing, i.e., more than 1 million nets, DGR becomes more efficient than CUGR2. The reason is that DGR can generate better initial routing because of its concurrent global optimization nature, and better initial routing can avoid unnecessary rip-up and reroute, which is the major runtime bottleneck when design becomes complex. 
For the results after detailed routing, Yan[14] did not release the detailed routing results,  so we can only compare with CUGR2 [2] and SPRoute 2.0 [8]. We conducted experiments on ISPD2019 benchmarks, compared with SPRoute2.0 [8] DGR achieves 3.13% fewer wire length, 2.91% fewer via count, and 3.34% more DRVs. Calculated using metric in ISPD19 contest, our score is 2.89% lower than SPRoute 2.0 [8] in detailed routing (lower is better).  Compared with CUGR2 [2], DGR achieved 0.4% less wire length, 0.28% more via counts, and almost the same DRVs. The score is 0.4% lower than CUGR2 [2]. We analyze the reason for more via counts is because current DGR is a concurrent algorithm in 2D search space, where the via counts are approximated by the number of turning points in the 2D plane, however, the approximation will be inaccurate, especially when the design has more congestions: congested g-cells will require the wires going through some higher layer, resulting in more vias than estimated. 3D DGR is one of our future directions. Moreover, we noted that the performance of DGR is strongly related with the complexity of the design, when the design is more complex, DGR will have better global routing and detailed routing performance: in our experiments, when there is no nets to do maze-routing, i.e., design is simple enough to be routed by sole pattern-routing, DGR results in ~0.5% worse score due to the rough estimation of vias, however, when the percentage of nets to do maze-routing goes to 0.86%, DGR gains a ~0.5% improvement, when the percentage goes to 1.53%, the improvement is goes to 1.12%.




Review #2
Have a Question to Authors? (Optional)
Can authors provide some insights/observations on using different learning parameters, e.g., optimizers, learning rate, learning schedule etc, and how they affect the overall solution quality?

We appreciate the questions from reviewer!
DGR is similar with traditional AI algorithms in many aspects, both of them try to solve some NP-problems, and need to avoid local optimum, the training is also based on the back-propagation algorithm. So, we are also interested in exploring the influence of these learning parameters. We conducted hundreds of experiments on 18_5m benchmark and explored the influence of these parameters to result quality (weighted score, wire length, via count, and the number of nets with overflows before maze-routing). For the learning rate, (figure given in TODO link), we can see that a learning rate between 0.1 and 1 has the best performance, this aligns with the case in AI algorithms: too small learning rates make the training process not converge, while too large learning rates make the training process unstable. For the optimizer, we tried SGD, Adam, RMSprop, and adagrad, and found that SGD is the worst while Adam, rmsprop, and adagrad has a similar performance:
RMSProp has a better wire length performance, adagrad has better via counts, while adam makes a balance between the two metric. 
This also aligns with their mathematical properties:
SGD uses a fixed learning rate for all parameters, which might not be suitable for this problem: obviously, different candidates and different nets have different importance, and should be treated differently. For example, the nets in the congested area should be treated more carefully. RMSprop effectively normalizes the learning rates by dividing the current gradient by the square root of the moving average of squared gradients, in other word, RMSprop allows for faster convergence due to its normalization mechanism in situations where Adagrad might struggle with diminishing learning rates. A faster convergence leads to a better wire length performance. However, as we discussed in the first question, our current 2D DGR may not be able to estimate the via counts accurately, so the via numbers may even increase when RMSprop converges more: current 2D DGR formulation cannot tell that via numbers might increase when the g-cell is congested. 
We also conducted experiements on the other benchmark, 18_8m, and the conclusion is consistent across benchmarks: learning rate between 0.1 to 1 is the best, and SGD is the worst optimizer in this task, while RMSprop converges faster than adagrad and adam, which leads to a better wire length performance but worse via counts.